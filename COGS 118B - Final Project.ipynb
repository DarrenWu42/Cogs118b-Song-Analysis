{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project focuses on analyzing the relationship between a song's genre and its other features (beats per minute, energy, valence, liveness). Using the results of this analysis, we will be able to determine to what extent the features of a song decide its genre. \n",
    "\n",
    "We reach our goal using two unsupervised machine learning algorithms (K-Means Clustering, Principal Component Analysis (PCA), and Gaussian Mixture Model (GMM)) with song data obtained from \"Spotify - All Time Top 2000s Mega Dataset\" taken from https://www.kaggle.com/iamsumat/spotify-top-2000s-mega-dataset. More information on the implementation of pre-processing of the data, K-means and Principal Component Analysis is available in the Methods section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music has a number of unique features that group similar songs together in different categories called genres. With the increasing role played by technology in the world of music, these characteristics become more and more distinct and digital. Therefore, in the world we live in today, we thought it would be incredibly relevant to use the unsupervised machine learning concept of clustering in an attempt to find out whether these characteristics truly decide which genre a song belongs to. For artists who create music, knowing this information will allow them to continue creating music as part of a group (genre) that they hope to belong to.\n",
    "\n",
    "Our motivation for selecting this dataset is because it had enough relevant data for us to work with (20 genres + 8 features + 1995 songs)\n",
    "\n",
    "Our motivation for using K-means, PCA and GMM is because we have learnt these concepts through the course of this class, and through implementation we hope to figure out whether these algorithms lead to similar results when performed on the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common use of machine learning with music revolves around the process of Specific Song Recommendation. While most of these algorithms initially existed as relational rather than accounting for the musical features of the songs, many new projects have come to fruition which focus on recommending new songs based on the qualities of the music users enjoy listening to more than the artist of the song. One such project can be seen at https://towardsdatascience.com/music-to-my-ears-an-unsupervised-approach-to-user-specific-song-recommendation-6c291acc2c12 which uses the Million Song Dataset that we also considered using for our project. However, we decided to go with the dataset we're using in our project because of easier computation and larger relevance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step we took in pre-processing involves the exploration of the data. The dataset we used comes with 1994 rows x 14 columns. We checked for missing values, there were none. \n",
    "\n",
    "Next, we extracted audio features within the dataset that are relevant to clustering (columns 4 to 12, but not including column 10) and displayed them with their distribution. Column 10 is length and is therefore is irrelevant, but the rest of the columns include Beats Per Minute (BPM), Energy, Danceability, Loudness (dB), Liveness, Valence, Acousticness and Speechiness. \n",
    "\n",
    "Moving forward, we extracted and displayed the 149 different genres in the dataset. However, 149 genres will lead to the creation of too many clusters. To combat this, we reduced the number of genres by grouping similar genres together (i.e., 'album rock' and 'alternative rock' are grouped under 'rock'). Additionally, we also pick the most represented genres in the dataset. This route resulted in an output of 20 genres. Subsequently, 113 rows were removed from the dataset which included songs not of a relevant genre.\n",
    "\n",
    "The resulting dataset has 1881 rows x 14 columns.\n",
    "\n",
    "Refer to code attached for implementation and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means is an algorithm that iterates between two steps. The first step involves finding cluster memberships of each datapoint while keeping the cluster centers fixed. These cluster centers are randomly initialized, and cluster memberships are assigned based on the least euclidian distance between a datapoint and a cluster center. The second step involves repositioning the cluster center to the mean center position of all datapoints assigned that specific cluster membership. The datapoint positions are kept fixed during this second step.\n",
    "\n",
    "In the context of our project, we executed K-Means two times: Pre-PCA and Post-PCA. The reason for using the algorithm both times is stated in the results section below. With regards to the implementation, the first step we took in the K-Means algorithm is initializing random centroids (cluster centers) in the 'centroids' variable. The next few steps occur within a while loop which runs until the algorithm converges (until an if statement that checks whether the centroid is moved by a value less than 10^-6 executes as true). Within the loop, first we calculate the Euclidian distance between each datapoint (feature) and centroid (stored in variable dist_mat). Next, we define 'rnk' which is a rank matrix which stores the cluster memberships of each datapoint by assigning each datapoint to the closest centroid. Within this matrix, the value corresponding to a datapoint and its assigned cluster center based on least distance is 1, whereas the value corresponding to that datapoint and all other cluster centers of farther distance is 0. The next step involves moving each centroid closer to the mean position of all the datapoints that are members of that specific centroid. This is done by calculating the average of all the datapoints in a specific cluster, and subsequently moving the relevant centroid closer to that average. We set the number of clusters to 10 (K = 10) where each cluster represents a genre. We stored the relevant features in 'Feats' (BPM, Energy, Danceability, Loudness, Liveness, Valence, Acousticness, Speechiness).\n",
    "\n",
    "Refer to code attached for implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) focuses on dimensionality reduction. Once we reduce the dimensionality of the dataset by removing the redundant dimensions (features), we're left with the features that are the most relevant to the genre which is our ultimate goal.\n",
    "\n",
    "\n",
    "In our project, we used PCA once, and used the results from PCA in a few different ways. The first few steps of PCA were to normalize the matrix by subtracting the mean, find the covariance matrix by multiplying the normalized matrix with its transpose, and obtain the eigenvectors and eigenvalues of the covariance matrix using the np.eig command. Using the normalized eigenvalues of the covariance matrix, we graphed a scree plot to show the top principle components that contributed the most to the overall variance of our matrix. We then used the eigenvectors to created a heatmap in order to see how much each of our previous variables contribute to each of our new principle componenets. After that, we multiplied the transpose of our eigenvector matrix with our normalized data matrix to get our new data matrix in the new principle components basis. We then used that new data matrix in our other two algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Mixture Models have a plethora of different applications. In the case of this project, GMM was used to cluster the unlabeled data. Gaussian Mixture models generally involve a superposition of many gaussian distributions. This algorithm takes advantage of Expectation-Maximization, which is a well known method to find the maximum-likelihood estimations for model parameters when the given data has latent variables, has missing data, or is incomplete.\n",
    "\n",
    "\n",
    "In the project, like K-Means, we opted to execute GMM two times, Pre-PCA and Post-PCA. Because we had already implimented K-Means and Principal Component Analysis by hand, GMM was a tertiary unsupervised algorithm that was not the main focus of the project, but to give more depth and to analyse how K-Means competes with other well-known algorithms.\n",
    "\n",
    "\n",
    "As a general explination of how GMM functions, we can think of it as a Expectation Maximization algorith for a mixture of Gaussians. There is an iterative search for the covariance, weights, and means of the unlabeled data.\n",
    "\n",
    "\n",
    "The generalized E.M algorithm steps are listed below:\n",
    "\n",
    "\n",
    "The Expectation step is used to compute the expected likelihood of the data with respect to any possible missing data.\n",
    "\n",
    "\n",
    "The Maximization step finds the maxmimum likehood vector given the observed data, as well as computing the PDF (Probability Density Function) for the unobserved data.\n",
    "\n",
    "\n",
    "This is then generalized to GMM, where:\n",
    "\n",
    "\n",
    "The Expectation step computes the weights for each Gaussian for each datapoint\n",
    "\n",
    "\n",
    "The Maximization step modifies the parameters of each Gaussian in order to be able to maximize the likelihood for the given data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When K-Means was implemented Pre-PCA, the cluster membership was very sporadic amongst the ten clusters when using all 8 features. We expected the cluster membership to group similar to the genre composition, however this was not the case. Subsequently, using the results from PCA, we ran K-Means Post-PCA as well. We used the top 3 features as concluded from our PCA algorithm hoping that we would achieve results closer to the original membership we had intended, however,  once again, the numbers were sporadic and inaccurate.   \n",
    "\n",
    "Original Membership of Data\n",
    "\n",
    "1 | rock | 857 2 | pop | 388 3 | adult standards | 123 4 | metal | 93 5 | indie | 79 6 | dutch cabaret | 51 7 | soul | 45 8 | permanent wave | 38 9 | british invasion | 36 10 | hip hop | 29\n",
    "\n",
    "K-Means clustered Membership of Data (Pre-PCA)\n",
    "\n",
    "1 | 316      \n",
    "2 | 231      \n",
    "3 | 183      \n",
    "4 | 178      \n",
    "5 | 173      \n",
    "6 | 163      \n",
    "7 | 153      \n",
    "8 | 137      \n",
    "9 | 119      \n",
    "10| 86\n",
    "\n",
    "K-Means clustered Membership of Data (Post-PCA)\n",
    "\n",
    "1 | 244      \n",
    "2 | 223      \n",
    "3 | 203      \n",
    "4 | 192      \n",
    "5 | 178      \n",
    "6 | 173      \n",
    "7 | 150      \n",
    "8 | 143      \n",
    "9 | 134      \n",
    "10| 99 \n",
    "\n",
    "To further analyze our results, we found the genre percentages in each cluster only for our results to yield a majority of 'rock' (39% - 56%) and 'pop' (17% - 30%)  dominating each cluster. This made it clear that our K-Means algorithm provided results that are not aligned with our initial hypothesis upon analysis as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the results from PCA in 2 different ways.\n",
    "\n",
    "\n",
    "The first way we used the results from PCA was to determine the features that contributed the most variability to our data. Looking at the heatmap, we observed that Energy contributed the most to PC1, BPM and Liveliness contributed the most to PC2, and Valence contributed the most to PC3. We determined that these 4 features were the most important ones to focus future analysis on. This seemed to work pretty well in determining what features to use.\n",
    "\n",
    "\n",
    "The second way we used the results from PCA was to change the basis of our original data, turning it from 8 dimensions to 3. This made it easier to visualize and we hoped it would represent enough of the variability of our data to use in our other algorithms. This didn't seem to work as well as we had hoped it would. Perhaps using a different diemnsionality reduction technique would've provided us with better results to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When GMM was implimented to cluster the unlabeled data, our first intuition was that it worked much better than K-Means. For example, with ten different clusters, the cluster membership in K-Means was very sporadic, and did not fit the original membership we had intended. However, with running the Gaussian Mixture Model clustering, our results Pre-PCA matched the membership much better with some slight variance\n",
    "\n",
    "\n",
    "Original Membership of Data\n",
    "\n",
    "\n",
    "1 | rock | 857\n",
    "2 | pop | 388\n",
    "3 | adult standards | 123\n",
    "4 | metal | 93\n",
    "5 | indie | 79\n",
    "6 | dutch cabaret | 51\n",
    "7 | soul | 45\n",
    "8 | permanent wave | 38\n",
    "9 | british invasion | 36\n",
    "10 | hip hop | 29\n",
    "\n",
    "\n",
    "GMM clustered Membership of Data\n",
    "\n",
    "\n",
    "1      |      758      \n",
    "2      |      410      \n",
    "3      |      125      \n",
    "4      |      116      \n",
    "5      |      106      \n",
    "6      |       71      \n",
    "7      |       57      \n",
    "8      |       41      \n",
    "9      |       35      \n",
    "10     |       20      \n",
    "\n",
    "\n",
    "That being said, after further investigation, we opted to assume that this was by pure chance. Once we found the genre percentages in each cluster, we saw that the cluster membership genres matched that of the K-Means algorithm, in which the data was not consistent with out intended results. For example, Cluster 1 had a near one to one correspondance with cluster 1 of the original data in terms of size. This would generally mean that the GMM clustered membership should of contained mainly rock songs. However, after analysis, this was not the case as only about 39 percent of the songs in the first cluster were rock.\n",
    "\n",
    "\n",
    "GMM cluster 1 results:\n",
    "\n",
    "\n",
    "rock (39%), pop (22%), hip hop (15%), dutch cabaret (10%), metal (5%), adult standards (5%), soul (5%), indie (0%), british invasion (0%), permanent wave (0%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With GMM and K-Means yielding results that were seemingly innacurate, we carried out an extra analysis of our dataset between the relationship of each genre and artist. Upon implementation of code to carry out this analysis (which can be seen attached to this report), we were left with results which grouped each artist with only 1 genre. With these results, we learnt that genre classification would be far more accurate if we used the artist rather than the features of a song.\n",
    "\n",
    "With regards to the goal of our project, we learnt that based on our dataset, the features of a song cannot really control the genre.\n",
    "\n",
    "If we had more time with this project, we would have definitely added an extension, or even modified our current project, to analyze the performance of each clustering algorithm with the use of the relationship between artist and the genre of the song as opposed to using the hollow relationship between the genre and song features. As stated previously, upon analysis, we concluded that each artist fits exactly one genre. This is a good enough reason to believe that our algorithm would yield results more in line with our original cluster membership which we intended.\n",
    "\n",
    "Another extension we would consider, if given more time, is to draw a timeline through the decades of the progression of factors that influence the genre of a song. We would use our unsupervised machine learning algorithms on a dataset that contains similar information, with the main difference being an extra column with the year of release. Using the year, we would carry out extra analysis that would provide us with information on how the level of influence from these factors (whether this be artists or song features such as BPM, Liveliness, etc) that determine the genre has changed over the years. We believe these results would be valuable to predict the important features of the music industry in the future based on understanding the past and current progression.\n",
    "\n",
    "The reason why it didn't work is explained by the analysis of the genre percentage composition within each cluster. In an ideal world, in alignment with our intended results, each unique cluster would have 100% membership of one speific genre and all other genres with 0% membership. This genre with 100% membership in one cluster will have 0% membership in all other clusters. However, as can be seen from our results, this was not the case. Rock and Pop dominated each cluster with regards to percentage composition. Therefore, this doesn't make our project results inconclusive, but rather we reach the conclusion that on this dataset, the features of the song don't really control the genre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing: Nour Yehia and Zakaria\n",
    "\n",
    "Gaussian Mixture Model : Zakaria\n",
    "\n",
    "K-Means Algorithm: Bryan and Manan\n",
    "\n",
    "Principal Component Analysis: Darren\n",
    "\n",
    "Report + Video Presentation: Rahul\n",
    "\n",
    "This is the generic structure to the divison of work for this project. The individual(s) named next to each module listed above were the leads for that respective module. However, consistent meetings and communication via discord facilitated the working of all team members together throughout all modules of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The github repository link containing our entire project including code:\n",
    "\n",
    "https://github.com/DarrenWu42/Cogs118b-Song-Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/iamsumat/spotify-top-2000s-mega-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/music-to-my-ears-an-unsupervised-approach-to-user-specific-song-recommendation-6c291acc2c12 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
